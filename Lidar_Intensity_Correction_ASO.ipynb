{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import laspy\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e81c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration parameters\n",
    "file_path = '/../skiles-group2/Lidar/ASO/Lidar_Intensity_Correction_refrange_plus10'\n",
    "input_laz_file = 'SBB_20170221_merge.laz'\n",
    "epsg_code = \"EPSG:32613\"\n",
    "trajectory_csv = \"sbet.csv\"\n",
    "reference_range = 1339\n",
    "min_scan_angle = -15\n",
    "max_scan_angle = 15\n",
    "\n",
    "def handle_file_errors(func):\n",
    "    \"\"\"\n",
    "    Decorator to handle errors and logging for file operations.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in {func.__name__}: {e}\")\n",
    "            return None\n",
    "    return wrapper\n",
    "\n",
    "@handle_file_errors\n",
    "def read_las_file(input_laz_file):\n",
    "    pipeline = pdal.Reader.las(filename=input_laz_file).pipeline()\n",
    "    pipeline.execute()\n",
    "    return pipeline.arrays[0].copy()\n",
    "\n",
    "class UnsupportedFilterTypeError(Exception):\n",
    "    \"\"\"Exception raised for unsupported filter types.\"\"\"\n",
    "    pass\n",
    "\n",
    "def apply_filter(arr, filter_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies a specified PDAL filter to a point cloud array.\n",
    "\n",
    "    :param arr: The input point cloud array.\n",
    "    :param filter_type: The type of PDAL filter to apply.\n",
    "    :param kwargs: Additional keyword arguments for the filter.\n",
    "    :return: The filtered point cloud array, or None if an error occurs or the result is empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filter_type == 'range':\n",
    "            pipeline = pdal.Filter.range(**kwargs).pipeline(arr)\n",
    "        elif filter_type == 'returns':\n",
    "            pipeline = pdal.Filter.returns(**kwargs).pipeline(arr)\n",
    "        elif filter_type == 'elm':\n",
    "            pipeline = pdal.Filter.elm().pipeline(arr)\n",
    "        elif filter_type == 'smrf':\n",
    "            pipeline = pdal.Filter.smrf(**kwargs).pipeline(arr)\n",
    "        elif filter_type == 'voxelcenternearestneighbor':\n",
    "            pipeline = pdal.Filter.voxelcenternearestneighbor(**kwargs).pipeline(arr)\n",
    "        elif filter_type == 'ferry':\n",
    "            pipeline = pdal.Filter.ferry(**kwargs).pipeline(arr)\n",
    "        else:\n",
    "            raise UnsupportedFilterTypeError(f\"Unsupported filter type: {filter_type}\")\n",
    "\n",
    "        pipeline.execute()\n",
    "        filtered_array = pipeline.arrays[0].copy()\n",
    "        if len(filtered_array) == 0:\n",
    "            logging.warning(f\"Point cloud is empty after applying {filter_type} filter\")\n",
    "            return None\n",
    "        return filtered_array\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error applying {filter_type} filter: {e}\")\n",
    "        return None\n",
    "\n",
    "def filter_by_gps_time(arr, start_time, end_time):\n",
    "    if start_time >= end_time:\n",
    "        logging.error(f\"Invalid time range: start_time ({start_time}) must be less than end_time ({end_time})\")\n",
    "        return None\n",
    "    if start_time < 0 or end_time < 0:\n",
    "        logging.error(f\"Invalid time range: start_time ({start_time}) and end_time ({end_time}) must be non-negative\")\n",
    "        return None\n",
    "\n",
    "    min_gps_time = np.min(arr['GpsTime'])\n",
    "    max_gps_time = np.max(arr['GpsTime'])\n",
    "    if start_time < min_gps_time or end_time > max_gps_time:\n",
    "        logging.error(f\"Time range [{start_time}, {end_time}] is outside the point cloud's GPS time range [{min_gps_time}, {max_gps_time}]\")\n",
    "        return None\n",
    "\n",
    "    return apply_filter(arr, 'range', limits=f'GpsTime[{start_time}:{end_time}]')\n",
    "\n",
    "def filter_single_returns(arr):\n",
    "    return apply_filter(arr, 'returns', groups='only')\n",
    "\n",
    "def filter_scan_angle(arr, min_angle, max_angle):\n",
    "    return apply_filter(arr, 'range', limits=f'ScanAngleRank[{min_angle}:{max_angle}]')\n",
    "\n",
    "def remove_noise_and_find_ground(arr):\n",
    "    arr = apply_filter(arr, 'elm')\n",
    "    return apply_filter(arr, 'smrf', slope='0.2', window='16', threshold='0.45', scalar='1.2')\n",
    "\n",
    "def thin_point_cloud(arr, cell_size):\n",
    "    return apply_filter(arr, 'voxelcenternearestneighbor', cell=cell_size)\n",
    "\n",
    "def add_dimensions(arr):\n",
    "    dimensions = '=>Range, =>Incidence, =>CorrIntens, =>Refl, =>GrainSize'\n",
    "    return apply_filter(arr, 'ferry', dimensions=dimensions)\n",
    "\n",
    "@handle_file_errors\n",
    "def write_las_file(arr, filename, epsg_code):\n",
    "    pipeline = pdal.Writer.las(\n",
    "        minor_version=4,\n",
    "        extra_dims='all',\n",
    "        a_srs=epsg_code,\n",
    "        filename=filename\n",
    "    ).pipeline(arr)\n",
    "    pipeline.execute()\n",
    "\n",
    "def process_point_cloud(name, start_time, end_time, input_laz_file, epsg_code):\n",
    "    arr = read_las_file(input_laz_file)\n",
    "    arr = filter_by_gps_time(arr, start_time, end_time)\n",
    "    arr = filter_single_returns(arr)\n",
    "    arr = filter_scan_angle(arr, min_scan_angle, max_scan_angle)\n",
    "    arr = remove_noise_and_find_ground(arr)\n",
    "    arr = thin_point_cloud(arr, '1.0')\n",
    "    arr = add_dimensions(arr)\n",
    "    gps_time_filename = f'{name}.las'\n",
    "    write_las_file(arr, gps_time_filename, epsg_code)\n",
    "\n",
    "def process_trajectory(flight_line_las, trajectory_csv):\n",
    "    output_trajectory_files = []\n",
    "    try:\n",
    "        trajectory_data = np.loadtxt(trajectory_csv, skiprows=1, delimiter=',')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading trajectory file {trajectory_csv}: {e}\")\n",
    "        return output_trajectory_files\n",
    "\n",
    "    for las_f_name in flight_line_las:\n",
    "        with open(las_f_name, \"rb+\") as f:\n",
    "            f.seek(6)\n",
    "            f.write(bytes([17, 0, 0, 0]))\n",
    "\n",
    "        pipeline = pdal.Reader.las(filename=las_f_name).pipeline() \n",
    "        pipeline.execute()\n",
    "        arr = pipeline.arrays[0]\n",
    "\n",
    "        # Filter the trajectory data based on the current LAS file's GPS time range\n",
    "        filtered_trajectory_data = trajectory_data[(trajectory_data[:,0] > arr['GpsTime'].min()) & (trajectory_data[:,0] < arr['GpsTime'].max())]\n",
    "\n",
    "        num_points = len(arr['GpsTime'])\n",
    "        num_times = len(filtered_trajectory_data[:,0])\n",
    "\n",
    "        output_trajectory_file = las_f_name[:-4] + '_trajectory.txt'\n",
    "        output_trajectory_files.append(output_trajectory_file)\n",
    "        with open(output_trajectory_file, 'w') as f:\n",
    "            np.savetxt(f, filtered_trajectory_data, header='GPSTime Easting Northing Elevation Roll Pitch Yaw', comments='')\n",
    "    \n",
    "\n",
    "    return output_trajectory_files\n",
    "    \n",
    "            \n",
    "@njit(fastmath=True)\n",
    "def normalize_intensity(gps_times, trajectory_times):\n",
    "    \"\"\"Find the nearest index in trajectory_times for each time in gps_times.\"\"\"\n",
    "    for t in gps_times:\n",
    "        i = np.searchsorted(trajectory_times, t)\n",
    "        if i == 0 or (i < len(trajectory_times) and np.abs(t - trajectory_times[i-1]) < np.abs(t - trajectory_times[i])):\n",
    "            if i - 1 >= 0:\n",
    "                yield i - 1\n",
    "            else:\n",
    "                yield i\n",
    "        else:\n",
    "            if i < len(trajectory_times):\n",
    "                yield i\n",
    "            else:\n",
    "                yield i - 1\n",
    "\n",
    "def cos_incidence_angle(X, Y, Z, n1, n2, n3):\n",
    "    \"\"\"Calculate the cosine of the incidence angle.\"\"\"\n",
    "    numerator = (-X * n1) + (-Y * n2) + (-Z * n3)\n",
    "    denominator = np.sqrt(X**2 + Y**2 + Z**2) * np.sqrt(n1**2 + n2**2 + n3**2)\n",
    "    return numerator / denominator\n",
    "\n",
    "def correct_intensity(raw_intensity, range_dist, reference_range, incidence):\n",
    "    \"\"\"Correct intensity based on range distance and incidence angle.\"\"\"\n",
    "    return (raw_intensity * np.square(range_dist)) / (np.square(reference_range) * np.cos(incidence))\n",
    "\n",
    "def read_trajectory_data(traj_file_name):\n",
    "    \"\"\"Read and return the trajectory data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        return np.loadtxt(traj_file_name, skiprows=1, delimiter=' ')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading trajectory file {traj_file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_range_and_incidence(arr, trajectory_data, indices):\n",
    "    \"\"\"Calculate the range and incidence angle for the point cloud.\"\"\"\n",
    "    arr = filter_normal(arr, knn=8)\n",
    "    X, Y, Z = arr['X'] - trajectory_data[indices,1], arr['Y'] - trajectory_data[indices,2], arr['Z'] - trajectory_data[indices,3]\n",
    "    R = np.sqrt(X**2 + Y**2 + Z**2)\n",
    "    theta = cos_incidence_angle(X, Y, Z, arr['NormalX'], arr['NormalY'], arr['NormalZ'])\n",
    "    return R, np.arccos(theta)\n",
    "\n",
    "def filter_by_incidence(arr, max_incidence):\n",
    "    return apply_filter(arr, 'range', limits=f'Incidence[:{max_incidence}]')\n",
    "\n",
    "def filter_normal(arr, knn):\n",
    "    pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"filters.normal\",\n",
    "                \"knn\": knn\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_json), arrays=[arr])\n",
    "    pipeline.execute()\n",
    "    return pipeline.arrays[0]\n",
    "\n",
    "\n",
    "def process_lidar_data(las_file_name, traj_file_name, proj_code):\n",
    "    \"\"\"Process LiDAR data to correct intensity and calculate incidence angle.\"\"\"\n",
    "    try:\n",
    "        arr = read_las_file(las_file_name)\n",
    "        trajectory_data = read_trajectory_data(traj_file_name)\n",
    "        if arr is None or trajectory_data is None:\n",
    "            return\n",
    "\n",
    "        indices = list(normalize_intensity(arr['GpsTime'], trajectory_data[:,0]))\n",
    "        arr['Range'], arr['Incidence'] = calculate_range_and_incidence(arr, trajectory_data, indices)\n",
    "\n",
    "        # Filter incidence to less than 0.698132 (40 degrees) \n",
    "        arr = filter_by_incidence(arr, max_incidence=0.698132)\n",
    "        \n",
    "\n",
    "        # Correct intensity\n",
    "        arr['CorrIntens'] = correct_intensity(arr['Intensity'], arr['Range'], reference_range, arr['Incidence'])\n",
    "\n",
    "        # Write to LAS file\n",
    "        corrected_filename = las_file_name[:-4] + '_corrected.las'\n",
    "        write_las_file(arr, corrected_filename, proj_code)\n",
    "        print('Completed for ' + las_file_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing LiDAR data for {las_file_name}: {e}\")\n",
    "\n",
    "        \n",
    "def process_all_point_clouds(gps_times_with_names, input_laz_file, epsg_code):\n",
    "    for item in gps_times_with_names:\n",
    "        process_point_cloud(\n",
    "            name=item[\"name\"],\n",
    "            start_time=item[\"start\"],\n",
    "            end_time=item[\"end\"],\n",
    "            input_laz_file=input_laz_file,\n",
    "            epsg_code=epsg_code\n",
    "        )\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Process each flight line\n",
    "        gps_times_with_names = [\n",
    "            {\"name\": \"sbet_h1\", \"start\": 236496.551092, \"end\": 236524.737629},\n",
    "            {\"name\": \"sbet_h2\", \"start\": 236335.333763, \"end\": 236377.978642},\n",
    "            {\"name\": \"sbet_h3\", \"start\": 236115.292836, \"end\": 236144.689638},\n",
    "            {\"name\": \"sbet_h4\", \"start\": 235961.777337, \"end\": 236003.877066},\n",
    "            {\"name\": \"sbet_h5\", \"start\": 235760.980922, \"end\": 235789.902605},\n",
    "            {\"name\": \"sbet_v6\", \"start\": 233880.192683, \"end\": 233923.682649},\n",
    "            {\"name\": \"sbet_v5\", \"start\": 234048.506281, \"end\": 234092.626409},\n",
    "            {\"name\": \"sbet_v4\", \"start\": 234156.020964, \"end\": 234201.541418},\n",
    "            {\"name\": \"sbet_v3\", \"start\": 234363.123551, \"end\": 234406.178455},\n",
    "            {\"name\": \"sbet_v2\", \"start\": 234480.93065, \"end\": 234524.115591},\n",
    "            {\"name\": \"sbet_v1\", \"start\": 234694.554843, \"end\": 234736.974615},\n",
    "        ]\n",
    "        process_all_point_clouds(gps_times_with_names, input_laz_file, epsg_code)\n",
    "            \n",
    "        # Process trajectories\n",
    "        flight_line_las = glob.glob('*.las')\n",
    "        if not flight_line_las:\n",
    "            logging.error(\"No LAS files found.\")\n",
    "            return\n",
    "        trajectory_times = process_trajectory(flight_line_las, trajectory_csv)\n",
    "        if len(flight_line_las) != len(trajectory_times):\n",
    "            logging.error(\"Mismatch between the number of LAS files and trajectory files.\")\n",
    "            return\n",
    "\n",
    "        for las_f_name, traj_f_name in zip(flight_line_las, trajectory_times):\n",
    "            process_lidar_data(las_f_name, traj_f_name, epsg_code)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main function: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42128297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge point clouds\n",
    "!pdal merge sbet_h1_corrected.las sbet_h2_corrected.las sbet_h3_corrected.las sbet_h4_corrected.las sbet_h5_corrected.las sbet_v1_corrected.las sbet_v2_corrected.las sbet_v3_corrected.las sbet_v4_corrected.las sbet_v5_corrected.las sbet_v6_corrected.las --writers.las.minor_version=4 --writers.las.extra_dims=all --writers.las.a_srs=\"EPSG:32613\" corrected_intensity.las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read new las file\n",
    "pipeline = pdal.Reader.las(\n",
    "    filename     = 'corrected_intensity.las'\n",
    ").pipeline() \n",
    "pipeline.execute()\n",
    "arr_gs = pipeline.arrays[0].copy()\n",
    "\n",
    "#Remove outliers beyond 3 SD of median\n",
    "pipeline = pdal.Filter.mad(\n",
    "    dimension = 'CorrIntens',\n",
    "    k = 3.0,\n",
    "    ).pipeline(arr_gs)\n",
    "pipeline.execute()\n",
    "arr_gs = pipeline.arrays[0].copy()\n",
    "\n",
    "#Normalize\n",
    "max_intens = np.max(arr_gs['CorrIntens'])\n",
    "def normalize(corrected_intensity):\n",
    "    norm = corrected_intensity / max_intens\n",
    "    return norm\n",
    "arr_gs['CorrIntens'] = normalize(arr_gs['CorrIntens'])\n",
    "\n",
    "#Output tiff\n",
    "pipeline        = pdal.Writer.gdal(\n",
    "    dimension   = 'CorrIntens',\n",
    "    output_type = 'idw',\n",
    "    resolution  = '1.0',\n",
    "    override_srs = 'EPSG:32613',\n",
    "    filename    = 'corrected_intensity.tif'\n",
    ").pipeline(arr_gs) \n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2fad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare corrected_intensity.tif and in situ measurements to determine scale factor\n",
    "scale_factor = -0.03\n",
    "\n",
    "def scale(corr_intens):\n",
    "    scaled = corr_intens + scale_factor\n",
    "    return scaled \n",
    "\n",
    "arr_gs['Refl'] = scale(arr_gs['CorrIntens'])\n",
    "\n",
    "#Filter reflectance values to only include snow surfaces\n",
    "pipeline = pdal.Filter.range(\n",
    "    limits = 'Refl[0.40:]'\n",
    "    ).pipeline(arr_gs)\n",
    "pipeline.execute()\n",
    "arr_gs = pipeline.arrays[0].copy()\n",
    "\n",
    "#Convert incidence angles from radians to degrees to match lookup table\n",
    "arr_gs['Incidence'] = np.rad2deg(arr_gs['Incidence'])\n",
    "\n",
    "#Export las file\n",
    "gps_time_filename = 'reflectance.las'\n",
    "pipeline = pdal.Writer.las(\n",
    "    minor_version=4,\n",
    "    extra_dims='all',\n",
    "    a_srs='EPSG:32613',\n",
    "    filename=gps_time_filename\n",
    ").pipeline(arr_gs)\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the lookup table\n",
    "lookup_table = pd.read_csv('brf_lidar_1064_2.csv', index_col='grain_size')\n",
    "\n",
    "#Read the point cloud\n",
    "las = laspy.read('reflectance.las')\n",
    "\n",
    "#Determine the closest incidence column and retrieve the grain size\n",
    "grain_sizes = []\n",
    "for i in range(len(las.points)):\n",
    "    incidence = las.Incidence[i]\n",
    "    corr_intens = las.Refl[i]\n",
    "    \n",
    "    # Find the closest incidence column\n",
    "    closest_inc_col = min(lookup_table.columns, key=lambda col: abs(float(col.split('_')[1]) - incidence))\n",
    "    \n",
    "    # Find the grain size with the closest reflectance value\n",
    "    closest_grain_size = lookup_table[closest_inc_col].sub(corr_intens).abs().idxmin()\n",
    "    \n",
    "    grain_sizes.append(closest_grain_size)\n",
    "\n",
    "#Assign the grain size\n",
    "las.GrainSize = grain_sizes\n",
    "\n",
    "#Write the modified point cloud\n",
    "las.write('grain_size.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clip grain_size.las to SBB boundary\n",
    "\n",
    "#Read the shapefile\n",
    "gdf = gpd.read_file('SBB_basin_poly_ASO3m.shp')\n",
    "\n",
    "#Extract the first geometry as WKT\n",
    "polygon_wkt = gdf.geometry[0].wkt\n",
    "\n",
    "#Define the PDAL pipeline\n",
    "clip_to_boundary = {\n",
    "    \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": \"grain_size.las\"\n",
    "            },\n",
    "        {\n",
    "            \"type\": \"filters.crop\",\n",
    "            \"polygon\": polygon_wkt\n",
    "        },\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": \"grain_size_clip.las\",\n",
    "                \"minor_version\": \"4\",\n",
    "                \"extra_dims\": \"all\",\n",
    "                \"a_srs\": \"EPSG:32613\"\n",
    "            }\n",
    "    ]\n",
    "}\n",
    "\n",
    "#Write the pipeline to a JSON file\n",
    "with open('clip_to_boundary.json', 'w') as f:\n",
    "    json.dump(clip_to_boundary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pdal pipeline clip_to_boundary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export TIFFs\n",
    "\n",
    "def read_las_file(filename):\n",
    "    pipeline = pdal.Reader.las(filename=filename).pipeline()\n",
    "    pipeline.execute()\n",
    "    return pipeline.arrays[0].copy()\n",
    "\n",
    "def write_tiff(arr, dimension, filename, srs='EPSG:32613', output_type='idw', resolution='1.0'):\n",
    "    pipeline = pdal.Writer.gdal(\n",
    "        dimension=dimension,\n",
    "        output_type=output_type,\n",
    "        resolution=resolution,\n",
    "        override_srs=srs,\n",
    "        filename=filename\n",
    "    ).pipeline(arr)\n",
    "    pipeline.execute()\n",
    "\n",
    "#Read the LAS file once\n",
    "las_file = 'grain_size_clip.las'\n",
    "arr_gs = read_las_file(las_file)\n",
    "\n",
    "#Write different dimensions to tiff\n",
    "dimensions = {\n",
    "    'Incidence': 'incidence.tif',\n",
    "    'CorrIntens' : 'corrected_intensity.tif',\n",
    "    'Refl': 'reflectance.tif',\n",
    "    'GrainSize': 'grain_size.tif'\n",
    "}\n",
    "\n",
    "for dimension, output_file in dimensions.items():\n",
    "    write_tiff(arr_gs, dimension, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61563ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
